# 5. Load the Craigslist data and then compute:
#
cl = read.csv("cl_rentals.csv")
# 5. Load the Craigslist data and then compute:
#
cl = read.csv("cl_rentals.csv")
cl = read.csv("cl_rentals.csv")
ggplot(wine_cv) + aes(x = price, y = points, color = region_1) +
geom_point() +
facet_wrap(~taster_name, ncol = 2)
cl = read.csv("cl_rentals.csv")
library(ggplot2)
ggplot(wine_cv)
country <- unique(wine$country)
wine = read.csv('wine_enthusiast_ranking.csv')
library(httr)
library(jsonlite)
library(xml2) # package for parsing xml documents
library(rvest) # package for web scraping
library(stringr)
url = "https://theaggie.org/category/features/"
doc = read_html(url)
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
parse_article_links = function(page) {
# Get article URLs
div = xml_find_all(page, "//div[@id = 'tdi_74']") # Error 1, The id in the website has change from 113 to 74
links = xml_find_all(div, ".//h3/a")
urls = xml_attr(links, "href")
title = xml_attr(links, "title")
# Get next page URL
nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
if (length(next_page) == 0) {
next_url = NA
} else {
next_url = xml_attr(next_page, "href")
}
# Using a list allows us to return two objects
list(urls = urls, next_url = next_url)
}
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.
url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1
# On the last page, the next URL will be `NA`.
while (!is.na(url) && i <= 5) { # Modify it so it loop only for 5 pages
# Download and parse the page.
page = read_html(url)
result = parse_article_links(page)
# Save the article URLs in the `article_urls` list. The variable `i` is the
# page number.
article_urls[[i]] = result$url
i = i + 1
# Set the URL to the next URL.
url = result$next_url
# Sleep for 1/30th of a second so that we never make more than 30 requests
# per second.
Sys.sleep(1/30)
}
article_urls
library(httr)
library(jsonlite)
library(xml2) # package for parsing xml documents
library(rvest) # package for web scraping
library(stringr)
url = "https://theaggie.org/category/features/"
doc = read_html(url)
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
parse_article_links = function(page) {
# Get article URLs
div = xml_find_all(page, "//div[@id = 'tdi_74']") # Error 1, The id in the website has change from 113 to 74
links = xml_find_all(div, ".//h3/a")
urls = xml_attr(links, "href")
title = xml_attr(links, "title")
# Get next page URL
nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
if (length(next_page) == 0) {
next_url = NA
} else {
next_url = xml_attr(next_page, "href")
}
# Using a list allows us to return two objects
list(urls = urls, next_url = next_url)
}
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.
url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1
# On the last page, the next URL will be `NA`.
while (!is.na(url) && i <= 5) { # Modify it so it loop only for 5 pages
# Download and parse the page.
page = read_html(url)
result = parse_article_links(page)
# Save the article URLs in the `article_urls` list. The variable `i` is the
# page number.
article_urls[[i]] = result$url
i = i + 1
# Set the URL to the next URL.
url = result$next_url
# Sleep for 1/30th of a second so that we never make more than 30 requests
# per second.
Sys.sleep(1/30)
}
print(article_urls)
for (page in article_urls) {
cat("Page Titles:\n")
cat(paste(page$titles, "\n", sep = "\n"))
}
library(httr)
library(jsonlite)
library(xml2) # package for parsing xml documents
library(rvest) # package for web scraping
library(stringr)
url = "https://theaggie.org/category/features/"
doc = read_html(url)
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
parse_article_links = function(page) {
# Get article URLs
div = xml_find_all(page, "//div[@id = 'tdi_74']") # Error 1, The id in the website has change from 113 to 74
links = xml_find_all(div, ".//h3/a")
urls = xml_attr(links, "href")
title = xml_attr(links, "title")
# Get next page URL
nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
if (length(next_page) == 0) {
next_url = NA
} else {
next_url = xml_attr(next_page, "href")
}
# Using a list allows us to return two objects
list(urls = urls, next_url = next_url)
}
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.
url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1
title
title = xml_attr(links, "title")
titles_list <- unlist(lapply(article_data, function(x) x$titles))
library(httr)
library(jsonlite)
library(xml2) # package for parsing xml documents
library(rvest) # package for web scraping
library(stringr)
url = "https://theaggie.org/category/features/"
doc = read_html(url)
xml_find_all(doc, "//div[contains(@class, 'td_block_inner')]")
parse_article_links = function(page) {
# Get article URLs
div = xml_find_all(page, "//div[@id = 'tdi_74']") # Error 1, The id in the website has change from 113 to 74
links = xml_find_all(div, ".//h3/a")
urls = xml_attr(links, "href")
title = xml_attr(links, "title")
# Get next page URL
nav = xml_find_all(page, "//div[contains(@class, 'page-nav')]")
next_page = xml_find_all(nav, ".//a[contains(@aria-label, 'next-page')]")
if (length(next_page) == 0) {
next_url = NA
} else {
next_url = xml_attr(next_page, "href")
}
# Using a list allows us to return two objects
list(urls = urls, next_url = next_url)
}
# NOTE: This code is likely to take a while to run, and is meant more for
# reading than for you to run and try out.
url = "https://theaggie.org/category/features/"
article_urls = list()
i = 1
# On the last page, the next URL will be `NA`.
while (!is.na(url) && i <= 5) { # Modify it so it loop only for 5 pages
# Download and parse the page.
page = read_html(url)
result = parse_article_links(page)
# Save the article URLs in the `article_urls` list. The variable `i` is the
# page number.
article_urls[[i]] = result$url
i = i + 1
# Set the URL to the next URL.
url = result$next_url
# Sleep for 1/30th of a second so that we never make more than 30 requests
# per second.
Sys.sleep(1/30)
}
titles_list <- unlist(lapply(article_data, function(x) x$titles))
titles_list <- unlist(lapply(article_urls, function(x) x$titles))
#install.packages(stringr)
library(stringr)
sample_text <- c("The event was held on 23/04/2021 and the next event will be on 05/10/2022.")
str_replace_all(sample_text,"(0[1-9]|[1-2][0-9]|3[0-1])/(0[1-9]|1[0-2])/(\\d{4})","\\3-\\2-\\1")
x <- "I am here. \nAm I alive"
print(x)
print(x)
cat(x)
x <- "I am here. \nAm I alive"
x <- str_replace_all(x, "\\n", "\n")
x
cat(x)
# Define the input string
input_string <- "He wanted to say hello but was afraid"
# Create a vector containing all the words in the string
word_vector <- str_split(input_string, "\\s+")[[1]]
# Use stringr to locate any occurrences of the string "hello" in the vector
hello_indices <- which(word_vector == "hello")
# Print the indices of occurrences
print(hello_indices)
# Input text variable
text <- "The event was held on 23/04/2021 and the next event will be on 05/10/2022."
# Define regular expression pattern to match dates in "dd/mm/yyyy" format
pattern <- "\\b(\\d{2})/(\\d{2})/(\\d{4})\\b"
# Replace dates matching the pattern with "yyyy-mm-dd" format
formatted_text <- str_replace_all(text, pattern, "\\3-\\2-\\1")
# Output the formatted text
print(formatted_text)
str_replace_all(sample_text,"(0[1-9]|[1-2][0-9]|3[0-1])/(0[1-9]|1[0-2])/(\\d{4})","\\3-\\2-\\1")
rgeom(8,0.65)
qnorm(0.03,0.1)
qnorm(0.94, mean=0.75,sd=0.6875)
qnorm(0.03,0,1)
rexp(8,1/32)
rexp(8,1/30)
rexp(8,1/32)
rexp(8,1/32)
new = sum(11.95,18.01, 21.69, 62.86, 83.81, 4.03,1.83 12.64)
new = sum(11.95,18.01, 21.69, 62.86, 83.81, 4.03,1.83 12.64)
new = sum(11.95,18.01, 21.69, 62.86, 83.81, 4.03,1.83, 12.64)
new
216.82/8
x = c(11.95,18.01, 21.69, 62.86, 83.81, 4.03,1.83, 12.64)
x
x = c(11.95,18.01,21.69,62.86,83.81,4.03,1.83,12.64)
x
calculate_score <- function(x) {
score <- (1/8) * sum((x - 27.1025)^2)
return(score)
}
result <- calculate_score(x)
print(result)
calculate_x <- function(x) {
score <- (1/8) * sum((x - 27.1025)^2)
return(score)
}
result <- calculate_x(x)
print(result)
y <- "She said, \"Hi!\""
cat(y)
# Define the input string
input_string <- "He wanted to say hello but was afraid"
# Create a vector containing all the words in the string
word_vector <- str_split(input_string, "\\s+")[[1]]
# Use stringr to locate any occurrences of the string "hello" in the vector
hello_indices <- which(word_vector == "hello")
# Print the indices of occurrences
print(hello_indices)
files <- readRDS("dtm.rds")
dtm <- readRDS("dtm.rds")
dtm$dimnames$Docs <- manifest$title
manifest <- readRDS("dtm.rds", row.names = 1)
View(dtm)
inspect(dtm)
dtm <- readRDS("dtm.rds")
inspect(dtm)
library(cluster)
library(tidyverse)
library(tokenizers)
library(tm)
dtm <- readRDS("dtm.rds")
inspect(dtm)
findFreqTerms(dtm, 1000)
findAssocs(dtm, "boat", .85)
writing <- findAssocs(dtm, "writing", .85)
writing[[1]][1:15]
term_counts <- as.matrix(dtm)
term_counts <- data.frame(sort(colSums(term_counts), decreasing = TRUE))
term_counts <- cbind(newColName = rownames(term_counts), term_counts)
colnames(term_counts) <- c("term", "count")
ggplot(
data = term_counts[1:50, ],
aes(
x = fct_reorder(term, -count),
y = count)
) +
geom_bar(stat = "identity") +
theme(
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
) +
labs(
title = "Top 50 words in 18 Nineteenth-Century Novels",
x = "Word",
y = "Count"
)
dtm_tfidf <- DocumentTermMatrix(
cleaned_corpus,
control = list(weighting = weightTfIdf)
)
dtm <- DocumentTermMatrix(cleaned_corpus)
manifest <- read.csv("C19_novels_manifest.csv", row.names = 1)
View(manifest)
str(manifest)
manifest$genre <- as.factor(manifest$genre)
# Transform DTM into a matrix and then a data frame for term counts
term_counts <- as.matrix(dtm)
term_counts <- data.frame(sort(colSums(term_counts), decreasing = TRUE))
term_counts <- cbind(term = rownames(term_counts), term_counts)
colnames(term_counts) <- c("term", "count")
# Plot top 50 terms in corpus
library(ggplot2)
ggplot(data = term_counts[1:50, ], aes(x = reorder(term, -count), y = count)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
labs(title = "Top 50 words in 18 Nineteenth-Century Novels", x = "Word", y = "Count")
# Calculate tf-idf scores
dtm_tfidf <- weightTfIdf(dtm)
# Get top terms using tf-idf scores
tfidf_counts <- as.matrix(dtm_tfidf)
tfidf_counts <- data.frame(sort(colSums(tfidf_counts), decreasing = TRUE))
tfidf_counts <- cbind(term = rownames(tfidf_counts), tfidf_counts)
colnames(tfidf_counts) <- c("term", "tfidf")
# Plot top 50 terms with tf-idf scores
ggplot(data = tfidf_counts[1:50, ], aes(x = reorder(term, -tfidf), y = tfidf)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
labs(title = "Words with the 50-highest tf--idf scores in 18 Nineteenth-Century Novels", x = "Word", y = "TF-IDF")
# Transform tf-idf DTM into a data frame
tfidf_df <- as.data.frame(as.matrix(dtm_tfidf))
colnames(tfidf_df) <- manifest$title
# Find unique terms in specific documents
unique_terms_dracula <- rownames(tfidf_df[order(tfidf_df$Dracula, decreasing = TRUE)[1:50], ])
print(unique_terms_dracula)
unique_terms_frankenstein <- rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing = TRUE)[1:50], ])
print(unique_terms_frankenstein)
unique_terms_sense_sensibility <- rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing = TRUE)[1:50], ])
print(unique_terms_sense_sensibility)
View(manifest)
# Find unique terms in specific documents
unique_terms_WutheringHeights <- rownames(tfidf_df[order(tfidf_df$WutheringHeights, decreasing = TRUE)[1:50], ])
print(unique_terms_WutheringHeights)
tfidf_df <- as.matrix(dtm_tfidf)
tfidf_df <- as.data.frame(t(tfidf_df))
colnames(tfidf_df) <- manifest$title
# Find unique terms in specific documents
unique_terms_WutheringHeights <- rownames(tfidf_df[order(tfidf_df$WutheringHeights, decreasing = TRUE)[1:50], ])
print(unique_terms_WutheringHeights)
cleaned_corpus <- tm_map(cleaned_corpus, str_remove_all, pattern = "\\'s", replacement = " "))
cleaned_corpus <- tm_map(cleaned_corpus, str_remove_all, pattern = "\\'s", replacement = " ")
findAssocs(dtm_tfidf, terms = "writing", corlimit = .85)
View(dtm)
View(dtm)
View(dtm)
View(dtm)
unique_terms_Frankenstein <- rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing = TRUE)[1:50], ])
print(unique_terms_Frankenstein)
print(Senseand_Sensibility)
Senseand_Sensibility <- rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing = TRUE)[1:50], ])
print(Senseand_Sensibility)
# Define the input string
input_string <- "He wanted to say hello but was afraid"
# Create a vector containing all the words in the string
word_vector <- str_split(input_string, "\\s+")[[1]]
# Use stringr to locate any occurrences of the string "hello" in the vector
hello_indices <- which(word_vector == "hello")
# Print the indices of occurrences
print(hello_indices)
cleaned_corpus <- tm_map(cleaned_corpus, str_remove_all, pattern = "\\'s", replacement = " ")
dtm_tfidf <- DocumentTermMatrix(
cleaned_corpus,
control = list(weighting = weightTfIdf)
)
cleaned_corpus <- lapply(cleaned_corpus, paste, collapse = " ")
cleaned_corpus <- tm_map(
corpus,
tokenize_words,
stopwords = stopwords('SMART'),
lowercase = TRUE,
strip_punct = TRUE,
strip_numeric = TRUE
)
corpus[[6]]
dtm <- lapply(dtm, paste, collapse = " ")
corpus <- Corpus(VectorSource(files))
corpus[[6]]
str_sub(corpus[[6]]$content, start = 1, end = 500)
frankenstein <- corpus[[6]]$content
frankenstein <- str_replace_all(frankenstein, pattern = "[^A-Za-z]", replacement = " ")
frankenstein <- str_split(frankenstein, " ")
frankenstein <- lapply(frankenstein, function(x) x[x != ""])
cleaned_corpus <- tm_map(
corpus,
tokenize_words,
stopwords = stopwords('SMART'),
lowercase = TRUE,
strip_punct = TRUE,
strip_numeric = TRUE
)
cleaned_corpus <- lapply(cleaned_corpus, paste, collapse = " ")
cleaned_corpus <- Corpus(VectorSource(cleaned_corpus))
saveRDS(cleaned_corpus, "C19_novels_cleaned.rds")
cleaned_corpus <- readRDS("C19_novels_cleaned.rds")
cleaned_corpus <- tm_map(cleaned_corpus, str_remove_all, pattern = "\\'s", replacement = " ")
unique_terms_WutheringHeights <- rownames(tfidf_df[order(tfidf_df$WutheringHeights, decreasing = TRUE)[1:50], ])
print(unique_terms_WutheringHeights)
unique_terms_Frankenstein <- rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing = TRUE)[1:50], ])
print(unique_terms_Frankenstein)
Senseand_Sensibility <- rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing = TRUE)[1:50], ])
print(Senseand_Sensibility)
cleaned_corpus <- tm_map(cleaned_corpus, str_remove_all, pattern = "\\'s", replacement = " ")
unique_terms_Dracula <- rownames(tfidf_df[order(tfidf_df$Dracula, decreasing = TRUE)[1:50], ])
print(unique_terms_FDracula)
unique_terms_Dracula <- rownames(tfidf_df[order(tfidf_df$Dracula, decreasing = TRUE)[1:50], ])
print(unique_terms_Dracula)
